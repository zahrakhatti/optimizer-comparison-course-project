{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since time did not allow, I just run for 10 epochs for fashion_mnist, and 50 epochs for cifar, but it still shows which setting is better. \n",
    "\n",
    "2. Don't look at the printings becuase I had to check for something and I printed out something else. But if you look at the files, the logging is for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set the random seed for reproducibility\n",
    "def set_seed(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Dataset Preparation (FashionMNIST and CIFAR10)\n",
    "transform_fashion = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "transform_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Load FashionMNIST\n",
    "fashion_mnist_train = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_fashion)\n",
    "fashion_mnist_test = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_fashion)\n",
    "\n",
    "# Load CIFAR10\n",
    "cifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n",
    "cifar_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n",
    "\n",
    "# Split datasets into training and validation sets\n",
    "train_size = int(0.8 * len(fashion_mnist_train))\n",
    "val_size = len(fashion_mnist_train) - train_size\n",
    "fashion_mnist_train, fashion_mnist_val = random_split(fashion_mnist_train, [train_size, val_size])\n",
    "\n",
    "\n",
    "train_size_cifar = int(0.8 * len(cifar_train))\n",
    "val_size_cifar = len(cifar_train) - train_size_cifar\n",
    "cifar_train, cifar_val = random_split(cifar_train, [train_size_cifar, val_size_cifar])\n",
    "\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "fashion_mnist_train_loader = DataLoader(fashion_mnist_train, batch_size=batch_size, shuffle=True)\n",
    "fashion_mnist_val_loader = DataLoader(fashion_mnist_val, batch_size=batch_size)\n",
    "fashion_mnist_test_loader = DataLoader(fashion_mnist_test, batch_size=batch_size)\n",
    "\n",
    "\n",
    "cifar_train_loader = DataLoader(cifar_train, batch_size=batch_size, shuffle=True)\n",
    "cifar_val_loader = DataLoader(cifar_val, batch_size=batch_size)\n",
    "cifar_test_loader = DataLoader(cifar_test, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10, input_size=28):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # We initialize fc1 with None here, and set it once we know the size in forward\n",
    "        self.fc1 = None\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        # Initialize the fc1 layer dynamically based on flattened size if not already set\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.size(1), 128).to(x.device)\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "fashion_mnist_model = SimpleCNN(in_channels=1, num_classes=10, input_size=28)\n",
    "cifar_model = SimpleCNN(in_channels=3, num_classes=10, input_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, csv_filename):\n",
    "    model.train()\n",
    "    losses = []  # Store training losses per epoch\n",
    "    best_val_accuracy = 0  # Track the best validation accuracy during training\n",
    "\n",
    "    with open(csv_filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['epoch', 'train_loss', 'grad_norm', 'val_accuracy'])\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            grad_norms = []\n",
    "            for images, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                grad_norm = sum((p.grad.norm() ** 2).item() for p in model.parameters() if p.grad is not None)\n",
    "                grad_norms.append(grad_norm)\n",
    "\n",
    "            # Calculate average training loss and gradient norm for the epoch\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            avg_grad_norm = sum(grad_norms) / len(grad_norms)\n",
    "            losses.append(train_loss)\n",
    "            \n",
    "            # Calculate validation accuracy for this epoch\n",
    "            val_accuracy = calculate_accuracy(model, val_loader)\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy  # Update best validation accuracy\n",
    "\n",
    "            # Write data to CSV\n",
    "            writer.writerow([epoch, train_loss, avg_grad_norm, val_accuracy])\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {train_loss:.4f}, Grad Norm: {avg_grad_norm:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return losses, best_val_accuracy \n",
    "\n",
    "# Accuracy calculation function\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning and going through all the optimization models and datasets are being coded in section 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Custom Implementations of Adagrad and Adam\n",
    "class CustomAdagrad:\n",
    "    def __init__(self, params, lr=0.01, epsilon=1e-8):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.squared_gradients = [torch.zeros_like(p) for p in self.params]\n",
    "\n",
    "    def step(self):\n",
    "        for i, param in enumerate(self.params):\n",
    "            if param.grad is None:\n",
    "                continue\n",
    "            grad = param.grad.data\n",
    "            self.squared_gradients[i] += grad ** 2\n",
    "            param.data -= self.lr * grad / (torch.sqrt(self.squared_gradients[i]) + self.epsilon)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "class CustomAdam:\n",
    "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "        self.m = [torch.zeros_like(p) for p in self.params]\n",
    "        self.v = [torch.zeros_like(p) for p in self.params]\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, param in enumerate(self.params):\n",
    "            if param.grad is None:\n",
    "                continue\n",
    "            grad = param.grad.data\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            param.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SGD on FashionMNIST with lr=0.01, momentum=None\n",
      "Epoch [1/10] Loss: 2.1677, Grad Norm: 91851.1619, Val Accuracy: 0.5457\n",
      "Epoch [2/10] Loss: 1.1511, Grad Norm: 4031981.6935, Val Accuracy: 0.7085\n",
      "Epoch [3/10] Loss: 0.7266, Grad Norm: 12239228.2205, Val Accuracy: 0.7491\n",
      "Epoch [4/10] Loss: 0.6394, Grad Norm: 18692607.1182, Val Accuracy: 0.7744\n",
      "Epoch [5/10] Loss: 0.5934, Grad Norm: 24157574.8056, Val Accuracy: 0.7896\n",
      "Epoch [6/10] Loss: 0.5612, Grad Norm: 29794282.6937, Val Accuracy: 0.7997\n",
      "Epoch [7/10] Loss: 0.5348, Grad Norm: 35424131.3419, Val Accuracy: 0.8154\n",
      "Epoch [8/10] Loss: 0.5141, Grad Norm: 41060839.7834, Val Accuracy: 0.8214\n",
      "Epoch [9/10] Loss: 0.4961, Grad Norm: 46536602.0518, Val Accuracy: 0.8242\n",
      "Epoch [10/10] Loss: 0.4806, Grad Norm: 52160637.3433, Val Accuracy: 0.8332\n",
      "Training SGD on FashionMNIST with lr=0.001, momentum=None\n",
      "Epoch [1/10] Loss: 2.2930, Grad Norm: 38142.9929, Val Accuracy: 0.1235\n",
      "Epoch [2/10] Loss: 2.2760, Grad Norm: 288143.8323, Val Accuracy: 0.1693\n",
      "Epoch [3/10] Loss: 2.2564, Grad Norm: 909871.9877, Val Accuracy: 0.2560\n",
      "Epoch [4/10] Loss: 2.2295, Grad Norm: 2180324.3026, Val Accuracy: 0.3405\n",
      "Epoch [5/10] Loss: 2.1897, Grad Norm: 4666285.0046, Val Accuracy: 0.4400\n",
      "Epoch [6/10] Loss: 2.1274, Grad Norm: 9552382.9674, Val Accuracy: 0.4893\n",
      "Epoch [7/10] Loss: 2.0286, Grad Norm: 19202621.6106, Val Accuracy: 0.5153\n",
      "Epoch [8/10] Loss: 1.8781, Grad Norm: 37752715.3874, Val Accuracy: 0.5534\n",
      "Epoch [9/10] Loss: 1.6772, Grad Norm: 70574233.4685, Val Accuracy: 0.6004\n",
      "Epoch [10/10] Loss: 1.4591, Grad Norm: 120514632.8391, Val Accuracy: 0.6529\n",
      "Training SGD on CIFAR10 with lr=0.01, momentum=None\n",
      "Epoch [1/10] Loss: 2.2952, Grad Norm: 3703.5980, Val Accuracy: 0.1473\n",
      "Epoch [2/10] Loss: 2.2661, Grad Norm: 51217.3352, Val Accuracy: 0.1940\n",
      "Epoch [3/10] Loss: 2.1903, Grad Norm: 313340.9356, Val Accuracy: 0.2434\n",
      "Epoch [4/10] Loss: 2.0835, Grad Norm: 1090482.3013, Val Accuracy: 0.2706\n",
      "Epoch [5/10] Loss: 1.9926, Grad Norm: 2404918.2564, Val Accuracy: 0.2932\n",
      "Epoch [6/10] Loss: 1.9239, Grad Norm: 4051960.0748, Val Accuracy: 0.3234\n",
      "Epoch [7/10] Loss: 1.8643, Grad Norm: 5938709.6444, Val Accuracy: 0.3464\n",
      "Epoch [8/10] Loss: 1.8085, Grad Norm: 8281324.7225, Val Accuracy: 0.3593\n",
      "Epoch [9/10] Loss: 1.7602, Grad Norm: 11262355.1668, Val Accuracy: 0.3815\n",
      "Epoch [10/10] Loss: 1.7165, Grad Norm: 14774020.2505, Val Accuracy: 0.3983\n",
      "Training SGD on CIFAR10 with lr=0.001, momentum=None\n",
      "Epoch [1/10] Loss: 2.3024, Grad Norm: 4941.2462, Val Accuracy: 0.1335\n",
      "Epoch [2/10] Loss: 2.3005, Grad Norm: 34778.5249, Val Accuracy: 0.1502\n",
      "Epoch [3/10] Loss: 2.2986, Grad Norm: 93792.5207, Val Accuracy: 0.1562\n",
      "Epoch [4/10] Loss: 2.2966, Grad Norm: 184107.6889, Val Accuracy: 0.1649\n",
      "Epoch [5/10] Loss: 2.2946, Grad Norm: 309921.5568, Val Accuracy: 0.1689\n",
      "Epoch [6/10] Loss: 2.2925, Grad Norm: 476542.1313, Val Accuracy: 0.1713\n",
      "Epoch [7/10] Loss: 2.2902, Grad Norm: 687073.9362, Val Accuracy: 0.1731\n",
      "Epoch [8/10] Loss: 2.2876, Grad Norm: 953842.8142, Val Accuracy: 0.1765\n",
      "Epoch [9/10] Loss: 2.2846, Grad Norm: 1290796.7585, Val Accuracy: 0.1789\n",
      "Epoch [10/10] Loss: 2.2813, Grad Norm: 1708289.5302, Val Accuracy: 0.1825\n",
      "\n",
      "Best Hyperparameters and Optimizer:\n",
      "Dataset: FashionMNIST\n",
      "Optimizer: SGD\n",
      "Learning Rate: 0.01\n",
      "Momentum: None\n",
      "Best Validation Accuracy: 0.8331666666666667\n"
     ]
    }
   ],
   "source": [
    "# Define criterion and hyperparameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rates = [0.01, 0.001]\n",
    "momentums = [0.9, 0.99]\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Define datasets\n",
    "datasets = {\n",
    "    \"FashionMNIST\": (fashion_mnist_train_loader, fashion_mnist_val_loader),\n",
    "    \"CIFAR10\": (cifar_train_loader, cifar_val_loader)  # Uncomment if CIFAR10 data loaders are defined\n",
    "}\n",
    "\n",
    "# Define optimizers, including custom implementations\n",
    "optimizers = {\n",
    "    \"SGD\": lambda params, lr: optim.SGD(params, lr=lr),\n",
    "    \"SGD with Momentum\": lambda params, lr, momentum: optim.SGD(params, lr=lr, momentum=momentum),\n",
    "    \"Adagrad\": lambda params, lr: optim.Adagrad(params, lr=lr),\n",
    "    \"Adam\": lambda params, lr: optim.Adam(params, lr=lr),\n",
    "    \"Custom Adagrad\": lambda params, lr: CustomAdagrad(params, lr=lr),\n",
    "    \"Custom Adam\": lambda params, lr: CustomAdam(params, lr=lr)\n",
    "}\n",
    "\n",
    "# Directory for storing results\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Initialize variables to store the best hyperparameters and validation accuracy\n",
    "best_overall_val_accuracy = 0\n",
    "best_overall_hyperparams = None\n",
    "best_overall_optimizer = None\n",
    "best_overall_dataset = None\n",
    "best_model = None\n",
    "\n",
    "# Loop through each dataset, optimizer, and hyperparameter combination\n",
    "for dataset_name, (train_loader, val_loader) in datasets.items():\n",
    "    for opt_name, opt_fn in optimizers.items():\n",
    "        for lr in learning_rates:\n",
    "            for momentum in (momentums if \"Momentum\" in opt_name else [None]):\n",
    "                model = SimpleCNN(in_channels=1 if dataset_name == \"FashionMNIST\" else 3, num_classes=10)\n",
    "                \n",
    "                # Initialize optimizer based on the presence of momentum\n",
    "                if opt_name == \"SGD with Momentum\":\n",
    "                    optimizer = opt_fn(model.parameters(), lr=lr, momentum=momentum)\n",
    "                else:\n",
    "                    optimizer = opt_fn(model.parameters(), lr=lr)\n",
    "                \n",
    "                # Generate the CSV filename with the dataset name, even if momentum is None\n",
    "                csv_filename = f'results/{opt_name}_lr_{lr}_momentum_{momentum}_{dataset_name}.csv'\n",
    "                \n",
    "                # Train the model and save results\n",
    "                print(f\"Training {opt_name} on {dataset_name} with lr={lr}, momentum={momentum}\")\n",
    "                losses, val_accuracy = train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, csv_filename)\n",
    "                \n",
    "                # Update best hyperparameters if the current validation accuracy is higher\n",
    "                if val_accuracy > best_overall_val_accuracy:\n",
    "                    best_overall_val_accuracy = val_accuracy\n",
    "                    best_overall_hyperparams = {'learning_rate': lr, 'momentum': momentum}\n",
    "                    best_overall_optimizer = opt_name\n",
    "                    best_overall_dataset = dataset_name\n",
    "                    best_model = model\n",
    "\n",
    "# Print the best hyperparameters and corresponding validation accuracy\n",
    "print(\"\\nBest Hyperparameters and Optimizer:\")\n",
    "print(f\"Dataset: {best_overall_dataset}\")\n",
    "print(f\"Optimizer: {best_overall_optimizer}\")\n",
    "print(f\"Learning Rate: {best_overall_hyperparams['learning_rate']}\")\n",
    "print(f\"Momentum: {best_overall_hyperparams['momentum']}\")\n",
    "print(f\"Best Validation Accuracy: {best_overall_val_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Happy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
